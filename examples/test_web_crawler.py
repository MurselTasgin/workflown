#!/usr/bin/env python3
"""
Test script for the web crawler module.
"""

import asyncio
import sys
from pathlib import Path

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

from toolbox.webscrapers import WebCrawler, CrawlConfig, CrawlStrategy, crawl_url, crawl_urls


async def test_single_page_crawl():
    """Test crawling a single page."""
    
    print("ğŸ•·ï¸ Testing Single Page Crawl")
    print("=" * 50)
    
    url = "https://www.akbank.com"
    
    print(f"ğŸ“„ Crawling: {url}")
    
    # Test with default config
    page = await crawl_url(url)
    
    print(f"   âœ… Success!")
    print(f"   â€¢ Title: {page.title}")
    print(f"   â€¢ Status Code: {page.status_code}")
    print(f"   â€¢ Content Length: {page.content_length}")
    print(f"   â€¢ Links Found: {len(page.links)}")
    print(f"   â€¢ Images Found: {len(page.images)}")
    print(f"   â€¢ Depth: {page.depth}")
    
    if page.error:
        print(f"   â€¢ Error: {page.error}")
    
    # Show first few links
    if page.links:
        print(f"   â€¢ First 3 links:")
        for i, link in enumerate(page.links[:3]):
            print(f"     {i+1}. {link}")


async def test_multi_page_crawl():
    """Test crawling multiple pages."""
    
    print("\nğŸ•·ï¸ Testing Multi-Page Crawl")
    print("=" * 50)
    
    urls = [
        "https://example.com",
        "https://httpbin.org/html"
    ]
    
    print(f"ğŸ“„ Crawling {len(urls)} pages:")
    for url in urls:
        print(f"   â€¢ {url}")
    
    # Test with custom config
    config = CrawlConfig(
        max_pages=5,
        max_depth=2,
        request_delay=0.5,
        timeout=15
    )
    
    result = await crawl_urls(urls, follow_links=False, config=config)
    
    print(f"\nğŸ“Š Crawl Results:")
    print(f"   â€¢ Total Pages: {result.total_pages}")
    print(f"   â€¢ Successful: {result.successful_pages}")
    print(f"   â€¢ Failed: {result.failed_pages}")
    print(f"   â€¢ Total Links Found: {result.total_links_found}")
    print(f"   â€¢ Total Images Found: {result.total_images_found}")
    print(f"   â€¢ Crawl Time: {result.crawl_time:.2f} seconds")
    
    # Show details for each page
    for i, page in enumerate(result.pages):
        print(f"\n   ğŸ“„ Page {i+1}: {page.url}")
        print(f"      â€¢ Title: {page.title}")
        print(f"      â€¢ Status: {page.status_code}")
        print(f"      â€¢ Content Length: {page.content_length}")
        print(f"      â€¢ Links: {len(page.links)}")
        print(f"      â€¢ Images: {len(page.images)}")
        print(f"      â€¢ Depth: {page.depth}")
        if page.error:
            print(f"      â€¢ Error: {page.error}")


async def test_link_following():
    """Test crawling with link following."""
    
    print("\nğŸ•·ï¸ Testing Link Following")
    print("=" * 50)
    
    start_urls = ["https://www.akbank.com"]
    
    print(f"ğŸ“„ Starting from: {start_urls[0]}")
    print(f"ğŸ”— Following links with limits...")
    
    # Test with link following
    config = CrawlConfig(
        max_pages=3,
        max_depth=1,
        request_delay=1.0,
        timeout=15,
        strategy=CrawlStrategy.BREADTH_FIRST
    )
    
    result = await crawl_urls(start_urls, follow_links=True, config=config)
    
    print(f"\nğŸ“Š Link Following Results:")
    print(f"   â€¢ Total Pages: {result.total_pages}")
    print(f"   â€¢ Successful: {result.successful_pages}")
    print(f"   â€¢ Failed: {result.failed_pages}")
    print(f"   â€¢ Total Links Found: {result.total_links_found}")
    print(f"   â€¢ Total Images Found: {result.total_images_found}")
    print(f"   â€¢ Crawl Time: {result.crawl_time:.2f} seconds")
    
    # Show pages by depth
    pages_by_depth = {}
    for page in result.pages:
        depth = page.depth
        if depth not in pages_by_depth:
            pages_by_depth[depth] = []
        pages_by_depth[depth].append(page)
    
    for depth in sorted(pages_by_depth.keys()):
        pages = pages_by_depth[depth]
        print(f"\n   ğŸ“„ Depth {depth} ({len(pages)} pages):")
        for page in pages:
            print(f"      â€¢ {page.url} - {page.title}")


async def test_crawler_with_context_manager():
    """Test using the crawler as a context manager."""
    
    print("\nğŸ•·ï¸ Testing Context Manager")
    print("=" * 50)
    
    config = CrawlConfig(
        max_pages=2,
        request_delay=0.5,
        timeout=15
    )
    
    async with WebCrawler(config) as crawler:
        print(f"ğŸ“„ Crawling with context manager...")
        
        page = await crawler.crawl_single_page("https://www.akbank.com")
        
        print(f"   âœ… Success!")
        print(f"   â€¢ Title: {page.title}")
        print(f"   â€¢ Status Code: {page.status_code}")
        print(f"   â€¢ Content Length: {page.content_length}")
        print(f"   â€¢ Links Found: {len(page.links)}")
        print(f"   â€¢ Images Found: {len(page.images)}")
    
    print(f"   âœ… Context manager cleanup completed")


async def test_custom_config():
    """Test custom crawler configuration."""
    
    print("\nğŸ•·ï¸ Testing Custom Configuration")
    print("=" * 50)
    
    # Custom configuration
    config = CrawlConfig(
        max_pages=3,
        max_depth=2,
        max_concurrent_requests=2,
        request_delay=0.5,
        timeout=10,
        strategy=CrawlStrategy.DEPTH_FIRST,
        allowed_domains=["example.com"],
        excluded_paths=["/admin", "/private"],
        user_agents=[
            "CustomBot/1.0",
            "TestCrawler/1.0"
        ]
    )
    
    print(f"ğŸ“‹ Custom Config:")
    print(f"   â€¢ Max Pages: {config.max_pages}")
    print(f"   â€¢ Max Depth: {config.max_depth}")
    print(f"   â€¢ Max Concurrent: {config.max_concurrent_requests}")
    print(f"   â€¢ Request Delay: {config.request_delay}s")
    print(f"   â€¢ Strategy: {config.strategy.value}")
    print(f"   â€¢ Allowed Domains: {config.allowed_domains}")
    print(f"   â€¢ Excluded Paths: {config.excluded_paths}")
    
    result = await crawl_urls(
        ["https://www.akbank.com"], 
        follow_links=True, 
        config=config
    )
    
    print(f"\nğŸ“Š Results with Custom Config:")
    print(f"   â€¢ Total Pages: {result.total_pages}")
    print(f"   â€¢ Successful: {result.successful_pages}")
    print(f"   â€¢ Failed: {result.failed_pages}")
    print(f"   â€¢ Crawl Time: {result.crawl_time:.2f} seconds")


async def main():
    """Run all tests."""
    
    print("ğŸ•·ï¸ Web Crawler Test Suite")
    print("=" * 60)
    
    try:
        await test_single_page_crawl()
        await test_multi_page_crawl()
        await test_link_following()
        await test_crawler_with_context_manager()
        await test_custom_config()
        
        print(f"\nğŸ‰ All tests completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 