#!/usr/bin/env python3
"""
Test script to demonstrate the different web scrapers.
"""

import asyncio
import sys
from pathlib import Path

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

from toolbox.webscrapers import ScraperManager


async def test_scrapers():
    """Test the scrapers."""
    
    # Initialize scraper manager
    manager = ScraperManager()
    
    try:
        print("ğŸ”§ Available Scrapers:")
        for name in manager.get_available_scrapers():
            scraper = manager.get_scraper(name)
            capabilities = scraper.get_capabilities()
            print(f"   â€¢ {name}")
            print(f"     - Available: {capabilities['available']}")
            print(f"     - Supports Markdown: {capabilities['supports_markdown']}")
            print(f"     - Description: {capabilities['description']}")
        
        print(f"\nğŸ“Š Total available scrapers: {len(manager.get_available_scrapers())}")
        
        # Test URL
        test_url = "https://example.com"
        
        print(f"\nğŸ§ª Testing scrapers with URL: {test_url}")
        
        # Test with best scraper
        print("\nğŸ¯ Testing with best scraper:")
        try:
            result = await manager.extract_with_best_scraper(
                url=test_url,
                extract_links=True,
                extract_images=True
            )
            print(f"   âœ… Success!")
            print(f"   â€¢ Title: {result.title}")
            print(f"   â€¢ Content length: {len(result.content)}")
            print(f"   â€¢ Markdown length: {len(result.markdown_content)}")
            print(f"   â€¢ Links: {len(result.links)}")
            print(f"   â€¢ Images: {len(result.images)}")
            print(f"   â€¢ Extraction method: {result.metadata.get('extraction_method', 'unknown')}")
        except Exception as e:
            print(f"   âŒ Failed: {str(e)}")
        
        # Test with all scrapers
        print("\nğŸ”¬ Testing with all scrapers:")
        try:
            results = await manager.extract_with_all_scrapers(
                url=test_url,
                extract_links=True,
                extract_images=True
            )
            
            for scraper_name, result in results.items():
                print(f"   ğŸ“„ {scraper_name}:")
                print(f"      â€¢ Title: {result.title}")
                print(f"      â€¢ Content length: {len(result.content)}")
                print(f"      â€¢ Markdown length: {len(result.markdown_content)}")
                print(f"      â€¢ Links: {len(result.links)}")
                print(f"      â€¢ Images: {len(result.images)}")
                print(f"      â€¢ Method: {result.metadata.get('extraction_method', 'unknown')}")
                if result.metadata.get('error'):
                    print(f"      â€¢ Error: {result.metadata['error']}")
        
        except Exception as e:
            print(f"   âŒ Failed: {str(e)}")
    
    finally:
        # Clean up all scrapers
        for scraper in manager.scrapers.values():
            await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(test_scrapers()) 