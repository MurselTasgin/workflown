#!/usr/bin/env python3
"""
Demo script for Firecrawl scraper - shows how it would work with proper setup.
"""

import asyncio
import sys
from pathlib import Path

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

from toolbox.webscrapers.firecrawl_scraper import FirecrawlScraper
from toolbox.webscrapers.beautifulsoup_scraper import BeautifulSoupScraper


async def demo_scraper_differences():
    """Demo the differences between local and API-based scrapers."""
    
    print("üî• Scraper Architecture Demo")
    print("=" * 50)
    
    # Test HTML content
    test_html = """
    <html>
        <head><title>Test Page</title></head>
        <body>
            <h1>Hello World</h1>
            <p>This is a test page with some content.</p>
            <a href="https://example.com">Link</a>
            <img src="https://example.com/image.jpg" />
        </body>
    </html>
    """
    
    test_url = "https://example.com"
    
    print(f"\nüìã Test Setup:")
    print(f"   ‚Ä¢ HTML Content: {len(test_html)} characters")
    print(f"   ‚Ä¢ URL: {test_url}")
    
    # Test BeautifulSoup (local scraper)
    print(f"\nüîß BeautifulSoup Scraper (Local):")
    print(f"   ‚Ä¢ Type: Local HTML parser")
    print(f"   ‚Ä¢ Uses: Pre-fetched HTML content")
    print(f"   ‚Ä¢ Process: Parses provided HTML")
    
    bs_scraper = BeautifulSoupScraper()
    if bs_scraper.is_available:
        try:
            result = await bs_scraper.extract_content(
                html_content=test_html,
                url=test_url,
                extract_links=True,
                extract_images=True
            )
            print(f"   ‚úÖ Success!")
            print(f"   ‚Ä¢ Content length: {len(result.content)}")
            print(f"   ‚Ä¢ Links found: {len(result.links)}")
            print(f"   ‚Ä¢ Images found: {len(result.images)}")
        except Exception as e:
            print(f"   ‚ùå Failed: {str(e)}")
    else:
        print(f"   ‚ùå Not available")
    
    # Test Firecrawl (API-based scraper)
    print(f"\nüî• Firecrawl Scraper (API-based):")
    print(f"   ‚Ä¢ Type: API-based content fetcher")
    print(f"   ‚Ä¢ Uses: Direct URL access via API")
    print(f"   ‚Ä¢ Process: Ignores HTML content, fetches from URL")
    
    firecrawl_scraper = FirecrawlScraper(api_key="demo_key")
    print(f"   ‚Ä¢ Available: {firecrawl_scraper.is_available}")
    print(f"   ‚Ä¢ API Key Configured: {firecrawl_scraper.get_capabilities().get('api_key_configured', False)}")
    print(f"   ‚Ä¢ Client Initialized: {firecrawl_scraper.get_capabilities().get('client_initialized', False)}")
    
    print(f"\nüìã Key Differences:")
    print(f"   ‚Ä¢ Local scrapers (BeautifulSoup, Trafilatura, etc.):")
    print(f"     - Receive pre-fetched HTML content")
    print(f"     - Parse the HTML to extract content")
    print(f"     - More efficient (no duplicate HTTP requests)")
    print(f"     - Work offline with provided HTML")
    print(f"   ‚Ä¢ API-based scrapers (Firecrawl):")
    print(f"     - Ignore provided HTML content")
    print(f"     - Fetch content directly from URL via API")
    print(f"     - Higher quality extraction (server-side processing)")
    print(f"     - Require API key and internet connection")
    
    print(f"\nüìã Setup Instructions:")
    print(f"   1. Install firecrawl: pip install firecrawl")
    print(f"   2. Add FIRECRAWL_API_KEY to your .env file:")
    print(f"      FIRECRAWL_API_KEY=your_api_key_here")
    print(f"   3. The scraper will automatically be used as the preferred option")
    print(f"   4. It will extract high-quality content with markdown support")


if __name__ == "__main__":
    asyncio.run(demo_scraper_differences()) 